{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDrdMzrpTGLu",
    "outputId": "36d84cf0-5849-48ac-b077-e5924a29c6bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Paths set: True True\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For Colab\n",
    "code_dir = \"/content/drive/MyDrive/ClimateLens\"\n",
    "data_dir = \"/content/drive/MyDrive/ClimateLens/data\"\n",
    "\n",
    "print('Paths set:', bool(code_dir), bool(data_dir))\n",
    "\n",
    "directories = {\n",
    "    \"emotions\": Path(code_dir) / \"visualizations\" / \"emotions\",\n",
    "    \"sentiments\": Path(code_dir) / \"visualizations\" / \"sentiments\",\n",
    "}\n",
    "\n",
    "for path in directories.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "emotion_dir = directories.get(\"emotions\")\n",
    "sentiment_dir = directories.get(\"sentiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el6heyPIMSO2"
   },
   "source": [
    "Emotion\n",
    "*   Word clouds to represent overall emotional distribution\n",
    "\n",
    "+ Time-series visualization to show how emotions change over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Dfx_rFI0MWYj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "outputId": "b853a10c-6056-4c4b-9436-0b342d4fb4ce"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please upload your CSV files:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-943ff21e-3ed8-4874-9af2-cb8416481420\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-943ff21e-3ed8-4874-9af2-cb8416481420\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving filtered_anticonsumption_comments.csv to filtered_anticonsumption_comments (5).csv\n",
      "Saving climate_twitter_sample.csv to climate_twitter_sample (5).csv\n",
      "\n",
      "Uploaded files: ['filtered_anticonsumption_comments (5).csv', 'climate_twitter_sample (5).csv']\n",
      "Twitter data loaded: 2736 rows\n",
      "Reddit data loaded: 2736 rows\n"
     ]
    }
   ],
   "source": [
    "# Upload files directly to Colab session\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"Please upload your CSV files:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the actual uploaded filenames (handles duplicates like \"(2)\")\n",
    "uploaded_files = list(uploaded.keys())\n",
    "print(f\"\\nUploaded files: {uploaded_files}\")\n",
    "\n",
    "# Find Twitter and Reddit files\n",
    "twitter_file = [f for f in uploaded_files if 'twitter' in f.lower()][0]\n",
    "reddit_file = [f for f in uploaded_files if 'reddit' in f.lower() or 'anticonsumption' in f.lower()][0]\n",
    "\n",
    "# Load the files\n",
    "twitter_df = pd.read_csv(io.BytesIO(uploaded[twitter_file]))\n",
    "reddit_df = pd.read_csv(io.BytesIO(uploaded[reddit_file]))\n",
    "\n",
    "# Create dictionary for easy iteration\n",
    "dfs = {\n",
    "    'Twitter': twitter_df,\n",
    "    'Reddit': reddit_df\n",
    "}\n",
    "\n",
    "print(f\"Twitter data loaded: {len(twitter_df)} rows\")\n",
    "print(f\"Reddit data loaded: {len(reddit_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrEd5PsOg_je"
   },
   "source": [
    "Sentiment Visualizations\n",
    "1. Distribution as Pie Charts\n",
    "2. Sentiment Probability Histograms\n",
    "3. Sentiment Probability Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PMpXaD8vg_jj"
   },
   "outputs": [],
   "source": [
    "sent_dist = Path(sentiment_dir) / \"pie charts\"\n",
    "sent_vio = Path(sentiment_dir) / \"violin distributions\"\n",
    "sent_his = Path(sentiment_dir) / \"probability histograms\"\n",
    "\n",
    "os.makedirs(sent_dist, exist_ok=True)\n",
    "os.makedirs(sent_vio, exist_ok=True)\n",
    "os.makedirs(sent_his, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "UFhqXmywg_jn",
    "outputId": "9db26175-cdc0-472e-d92b-6bb7652e3579"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'sentiment_label'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3805\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3806\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mindex.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mindex.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'sentiment_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-4105672282.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdfs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# saving each seperately\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mcreate_pie_plot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     \u001B[0mfile_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{name}_sentiment_pie.png\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdpi\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-4105672282.py\u001B[0m in \u001B[0;36mcreate_pie_plot\u001B[0;34m(df, title)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcreate_pie_plot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtitle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0msentiment_counts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'sentiment_label'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue_counts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     sentiment_counts.plot(\n\u001B[1;32m      6\u001B[0m         \u001B[0mkind\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'pie'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4100\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4101\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4102\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4103\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4104\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3810\u001B[0m             ):\n\u001B[1;32m   3811\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mInvalidIndexError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3812\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3813\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3814\u001B[0m             \u001B[0;31m# If we have a listlike key, _check_indexing_error will raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'sentiment_label'"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "save_dir = sent_dist\n",
    "\n",
    "def create_pie_plot(df,title):\n",
    "    sentiment_counts = df['sentiment_label'].value_counts()\n",
    "    sentiment_counts.plot(\n",
    "        kind='pie',\n",
    "        autopct='%1.1f%%',\n",
    "        colors=['skyblue', 'lightcoral', 'lightgreen'],\n",
    "        labels=sentiment_counts.index\n",
    "    )\n",
    "    plt.title(title)\n",
    "\n",
    "for name, df in dfs.items(): # saving each seperately\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    create_pie_plot(df, name)\n",
    "    file_path = os.path.join(save_dir, f\"{name}_sentiment_pie.png\")\n",
    "    plt.savefig(file_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "# one big combined visual\n",
    "n_datasets = len(dfs)\n",
    "plt.figure(figsize=(6, 4 * n_datasets))\n",
    "\n",
    "for idx, (name, df) in enumerate(dfs.items(), start=1):\n",
    "    plt.subplot(n_datasets, 1, idx)\n",
    "    create_pie_plot(df, name)\n",
    "\n",
    "plt.tight_layout()\n",
    "combined_path = os.path.join(save_dir, \"all_sentiment_pies.png\")\n",
    "plt.savefig(combined_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Combined visualization saved to: {combined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLG-q5sCg_jp"
   },
   "source": [
    "## **Sentiment Probability Distribution by Sentiment Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "0KF0V9F_g_jp",
    "outputId": "39ca0a62-45a2-465e-cf00-1b4b396534e3"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Could not interpret value `sentiment_proba` for `x`. An entry with this name does not appear in `data`.",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-4051589135.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdfs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0mcreate_violin_plot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0mfile_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{name}_sentiment_violin.png\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdpi\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-4051589135.py\u001B[0m in \u001B[0;36mcreate_violin_plot\u001B[0;34m(df, title)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcreate_violin_plot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtitle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     sns.violinplot(\n\u001B[0m\u001B[1;32m      5\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'sentiment_proba'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/categorical.py\u001B[0m in \u001B[0;36mviolinplot\u001B[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, inner, split, width, dodge, gap, linewidth, linecolor, cut, gridsize, bw_method, bw_adjust, density_norm, common_norm, hue_norm, formatter, log_scale, native_scale, legend, scale, scale_hue, bw, inner_kws, ax, **kwargs)\u001B[0m\n\u001B[1;32m   1723\u001B[0m ):\n\u001B[1;32m   1724\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1725\u001B[0;31m     p = _CategoricalPlotter(\n\u001B[0m\u001B[1;32m   1726\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1727\u001B[0m         \u001B[0mvariables\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/categorical.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001B[0m\n\u001B[1;32m     65\u001B[0m     ):\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0;31m# This method takes care of some bookkeeping that is necessary because the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    632\u001B[0m         \u001B[0;31m# information for numeric axes would be information about log scales.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    633\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_var_ordered\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m\"x\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"y\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m}\u001B[0m  \u001B[0;31m# alt., used DefaultDict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 634\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0massign_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    635\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m         \u001B[0;31m# TODO Lots of tests assume that these are called to initialize the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001B[0m in \u001B[0;36massign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    677\u001B[0m             \u001B[0;31m# to centralize / standardize data consumption logic.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    678\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_format\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"long\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 679\u001B[0;31m             \u001B[0mplot_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPlotData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    680\u001B[0m             \u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplot_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    681\u001B[0m             \u001B[0mnames\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplot_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhandle_data_source\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_assign_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001B[0m in \u001B[0;36m_assign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    230\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m                     \u001B[0merr\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m\"An entry with this name does not appear in `data`.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    233\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    234\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Could not interpret value `sentiment_proba` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "save_dir=sent_vio\n",
    "\n",
    "def create_violin_plot(df, title):\n",
    "    sns.violinplot(\n",
    "        data=df,\n",
    "        x='sentiment_proba',\n",
    "        y='sentiment_label',\n",
    "        inner='box',\n",
    "        palette='husl',\n",
    "        hue='sentiment_label'\n",
    "    )\n",
    "    sns.despine(top=True, right=True, bottom=True, left=True)\n",
    "    plt.title(title)\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    create_violin_plot(df, name)\n",
    "    file_path = os.path.join(save_dir, f\"{name}_sentiment_violin.png\")\n",
    "    plt.savefig(file_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "n_datasets = len(dfs)\n",
    "plt.figure(figsize=(8, 6 * n_datasets))\n",
    "\n",
    "for idx, (name, df) in enumerate(dfs.items(), start=1):\n",
    "    plt.subplot(n_datasets, 1, idx)\n",
    "    create_violin_plot(df, name)\n",
    "\n",
    "plt.tight_layout()\n",
    "combined_path = os.path.join(save_dir, \"all_sentiment_violins.png\")\n",
    "plt.savefig(combined_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Combined visualization saved to: {combined_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "rzW896I5g_jr",
    "outputId": "49d71c7f-9953-4d85-91a6-9974fde42cf5"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Could not interpret value `sentiment_proba` for `x`. An entry with this name does not appear in `data`.",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-3114583516.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdfs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m     \u001B[0mcreate_histplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m     \u001B[0mfile_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{name}_sentiment_hist.png\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdpi\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-3114583516.py\u001B[0m in \u001B[0;36mcreate_histplot\u001B[0;34m(df, title)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcreate_histplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtitle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     sns.histplot(\n\u001B[0m\u001B[1;32m      5\u001B[0m         \u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'sentiment_proba'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0mhue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'sentiment_label'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/distributions.py\u001B[0m in \u001B[0;36mhistplot\u001B[0;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001B[0m\n\u001B[1;32m   1377\u001B[0m ):\n\u001B[1;32m   1378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1379\u001B[0;31m     p = _DistributionPlotter(\n\u001B[0m\u001B[1;32m   1380\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1381\u001B[0m         \u001B[0mvariables\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/distributions.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    108\u001B[0m     ):\n\u001B[1;32m    109\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    632\u001B[0m         \u001B[0;31m# information for numeric axes would be information about log scales.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    633\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_var_ordered\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m\"x\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"y\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m}\u001B[0m  \u001B[0;31m# alt., used DefaultDict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 634\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0massign_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    635\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m         \u001B[0;31m# TODO Lots of tests assume that these are called to initialize the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001B[0m in \u001B[0;36massign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    677\u001B[0m             \u001B[0;31m# to centralize / standardize data consumption logic.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    678\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_format\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"long\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 679\u001B[0;31m             \u001B[0mplot_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPlotData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    680\u001B[0m             \u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplot_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    681\u001B[0m             \u001B[0mnames\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplot_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhandle_data_source\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_assign_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001B[0m in \u001B[0;36m_assign_variables\u001B[0;34m(self, data, variables)\u001B[0m\n\u001B[1;32m    230\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m                     \u001B[0merr\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m\"An entry with this name does not appear in `data`.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    233\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    234\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Could not interpret value `sentiment_proba` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 0 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "save_dir=sent_his\n",
    "\n",
    "def create_histplot(df, title):\n",
    "    sns.histplot(\n",
    "        x='sentiment_proba',\n",
    "        hue='sentiment_label',\n",
    "        data=df,\n",
    "        element='step'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Sentiment Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    create_histplot(df, name)\n",
    "    file_path = os.path.join(save_dir, f\"{name}_sentiment_hist.png\")\n",
    "    plt.savefig(file_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "n_datasets = len(dfs)\n",
    "plt.figure(figsize=(12, 4 * n_datasets))\n",
    "\n",
    "for idx, (name, df) in enumerate(dfs.items(), start=1):\n",
    "    plt.subplot(n_datasets, 1, idx)\n",
    "    create_histplot(df, name)\n",
    "\n",
    "plt.tight_layout()\n",
    "combined_path = os.path.join(save_dir, \"all_sentiment_hists.png\")\n",
    "plt.savefig(combined_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Combined visualization saved to: {combined_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "!pip install -q wordcloud plotly transformers torch kaleido\n",
    "print(\" Packages installed\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3v5a2vl5WBXL",
    "outputId": "24c2bc27-5b9b-484f-a6e5-1618f4b31313"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Packages installed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP & Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "print(\" Libraries imported\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRmCv-7ZWoi7",
    "outputId": "c5d08100-ff76-4262-b641-a4795648d294"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Libraries imported\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load emotion detection model\n",
    "print(\"Loading emotion detection model...\")\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "\n",
    "# Check for GPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Initialize pipeline\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    device=device,\n",
    "    top_k=None  # Return all emotion scores\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Emotions detected: anger, disgust, fear, joy, neutral, sadness, surprise\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b_mfN6cWtmW",
    "outputId": "c4a70bcf-e867-48f0-c980-0964be81f954"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading emotion detection model...\n",
      "Using device: CPU\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loaded successfully\n",
      "Emotions detected: anger, disgust, fear, joy, neutral, sadness, surprise\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def detect_emotions_batch(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Detect emotions for a list of texts with batch processing.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        batch_size: Number of texts to process at once\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with emotion labels and scores\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        # Truncate long texts (model limit: 512 tokens)\n",
    "        batch = [text[:512] if isinstance(text, str) else \"\" for text in batch]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = emotion_classifier(batch)\n",
    "\n",
    "        # Extract top emotion for each text\n",
    "        for pred in predictions:\n",
    "            # pred is a list of dicts with 'label' and 'score'\n",
    "            top_emotion = max(pred, key=lambda x: x['score'])\n",
    "\n",
    "            # Create dict with all emotion scores\n",
    "            emotion_scores = {item['label']: item['score'] for item in pred}\n",
    "\n",
    "            results.append({\n",
    "                'emotion_label': top_emotion['label'],\n",
    "                'emotion_confidence': top_emotion['score'],\n",
    "                **emotion_scores  # Add individual emotion scores\n",
    "            })\n",
    "\n",
    "        # Progress indicator\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(texts))}/{len(texts)} texts...\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\" Emotion detection function ready\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CMzrVl1XCqi",
    "outputId": "3ebbb7a8-c039-44fb-ea95-16f8edd604a2"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Emotion detection function ready\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DETECTING EMOTIONS - This may take a few minutes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Twitter emotions\n",
    "print(\"\\nProcessing Twitter data...\")\n",
    "twitter_emotions = detect_emotions_batch(twitter_df['cleaned_text'].fillna('').tolist())\n",
    "twitter_df = pd.concat([twitter_df, twitter_emotions], axis=1)\n",
    "print(f\" Twitter: {len(twitter_df)} rows processed\")\n",
    "print(f\" Emotion distribution:\\n{twitter_df['emotion_label'].value_counts()}\\n\")\n",
    "\n",
    "# Reddit emotions\n",
    "print(\"\\n Processing Reddit data...\")\n",
    "reddit_emotions = detect_emotions_batch(reddit_df['cleaned_text'].fillna('').tolist())\n",
    "reddit_df = pd.concat([reddit_df, reddit_emotions], axis=1)\n",
    "print(f\" Reddit: {len(reddit_df)} rows processed\")\n",
    "print(f\" Emotion distribution:\\n{reddit_df['emotion_label'].value_counts()}\\n\")\n",
    "\n",
    "# Update dfs dictionary\n",
    "dfs = {\n",
    "    'Twitter': twitter_df,\n",
    "    'Reddit': reddit_df\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" EMOTION DETECTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AMz2jTDXG8Q",
    "outputId": "fd762b1b-423c-439f-fbc7-0803b97b0199"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "DETECTING EMOTIONS - This may take a few minutes...\n",
      "============================================================\n",
      "\n",
      "Processing Twitter data...\n",
      "Processed 32/2736 texts...\n",
      "Processed 352/2736 texts...\n",
      "Processed 672/2736 texts...\n",
      "Processed 992/2736 texts...\n",
      "Processed 1312/2736 texts...\n",
      "Processed 1632/2736 texts...\n",
      "Processed 1952/2736 texts...\n",
      "Processed 2272/2736 texts...\n",
      "Processed 2592/2736 texts...\n",
      " Twitter: 2736 rows processed\n",
      " Emotion distribution:\n",
      "emotion_label\n",
      "neutral     1431\n",
      "fear         309\n",
      "joy          301\n",
      "sadness      265\n",
      "anger        216\n",
      "disgust      109\n",
      "surprise     105\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      " Processing Reddit data...\n",
      "Processed 32/2736 texts...\n",
      "Processed 352/2736 texts...\n",
      "Processed 672/2736 texts...\n",
      "Processed 992/2736 texts...\n",
      "Processed 1312/2736 texts...\n",
      "Processed 1632/2736 texts...\n",
      "Processed 1952/2736 texts...\n",
      "Processed 2272/2736 texts...\n",
      "Processed 2592/2736 texts...\n",
      " Reddit: 2736 rows processed\n",
      " Emotion distribution:\n",
      "emotion_label\n",
      "anger       822\n",
      "neutral     631\n",
      "sadness     414\n",
      "fear        323\n",
      "joy         310\n",
      "surprise    168\n",
      "disgust      68\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      " EMOTION DETECTION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_emotion_wordclouds(df, dataset_name, save_dir):\n",
    "    \"\"\"\n",
    "    Create word clouds for each emotion and save as HTML.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'cleaned_text' and 'emotion_label' columns\n",
    "        dataset_name: Name for the dataset (e.g., 'Twitter', 'Reddit')\n",
    "        save_dir: Directory to save HTML files\n",
    "    \"\"\"\n",
    "    emotions = df['emotion_label'].unique()\n",
    "\n",
    "    # Color schemes for emotions\n",
    "    emotion_colors = {\n",
    "        'anger': 'Reds',\n",
    "        'disgust': 'Greens',\n",
    "        'fear': 'Purples',\n",
    "        'joy': 'YlOrRd',\n",
    "        'neutral': 'Greys',\n",
    "        'sadness': 'Blues',\n",
    "        'surprise': 'Oranges'\n",
    "    }\n",
    "\n",
    "    print(f\"\\n Creating word clouds for {dataset_name}...\")\n",
    "\n",
    "    # Create word cloud for each emotion\n",
    "    for emotion in sorted(emotions):\n",
    "        emotion_texts = df[df['emotion_label'] == emotion]['cleaned_text'].fillna('')\n",
    "        combined_text = ' '.join(emotion_texts.tolist())\n",
    "\n",
    "        if len(combined_text.strip()) == 0:\n",
    "            print(f\"    Skipping {emotion} - no text data\")\n",
    "            continue\n",
    "\n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            background_color='white',\n",
    "            colormap=emotion_colors.get(emotion, 'viridis'),\n",
    "            max_words=100,\n",
    "            relative_scaling=0.5,\n",
    "            min_font_size=10\n",
    "        ).generate(combined_text)\n",
    "\n",
    "        # Create matplotlib figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'{dataset_name} - {emotion.capitalize()} Emotion Word Cloud',\n",
    "                     fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "        # Save as PNG\n",
    "        filename = f\"{dataset_name.lower()}_{emotion}_wordcloud.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"   Saved: {filename}\")\n",
    "\n",
    "    # Create combined word cloud (all emotions)\n",
    "    all_text = ' '.join(df['cleaned_text'].fillna('').tolist())\n",
    "    wordcloud_all = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=150,\n",
    "        relative_scaling=0.5,\n",
    "        min_font_size=10\n",
    "    ).generate(all_text)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.imshow(wordcloud_all, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{dataset_name} - All Emotions Combined',\n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "    filename = f\"{dataset_name.lower()}_all_emotions_wordcloud.png\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"   Saved: {filename}\")\n",
    "    print(f\" {dataset_name} word clouds complete\\n\")\n",
    "\n",
    "print(\" Word cloud function ready\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqSOmT31Xa2V",
    "outputId": "f375a380-8d57-424a-df47-345221645b12"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Word cloud function ready\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_emotion_timeseries(df, dataset_name, time_column, save_dir):\n",
    "    \"\"\"\n",
    "    Create interactive time-series visualization of emotion trends.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with emotion data\n",
    "        dataset_name: Name for the dataset\n",
    "        time_column: Name of the time column\n",
    "        save_dir: Directory to save HTML files\n",
    "    \"\"\"\n",
    "    print(f\"\\n Creating time-series for {dataset_name}...\")\n",
    "\n",
    "    # Prepare time column\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    if dataset_name == 'Twitter':\n",
    "        # Parse Twitter datetime string\n",
    "        df_plot['datetime'] = pd.to_datetime(df_plot[time_column])\n",
    "        # Use hourly aggregation for short time span\n",
    "        df_plot['time_period'] = df_plot['datetime'].dt.floor('H')\n",
    "        time_format = '%Y-%m-%d %H:%M'\n",
    "    else:  # Reddit\n",
    "        # Convert unix timestamp to datetime\n",
    "        df_plot['datetime'] = pd.to_datetime(df_plot[time_column], unit='s')\n",
    "        # Use monthly aggregation for long time span\n",
    "        df_plot['time_period'] = df_plot['datetime'].dt.to_period('M').dt.to_timestamp()\n",
    "        time_format = '%Y-%m'\n",
    "\n",
    "    # Count emotions by time period\n",
    "    emotion_counts = df_plot.groupby(['time_period', 'emotion_label']).size().reset_index(name='count')\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_by_period = emotion_counts.groupby('time_period')['count'].transform('sum')\n",
    "    emotion_counts['percentage'] = (emotion_counts['count'] / total_by_period * 100).round(2)\n",
    "\n",
    "    # Create interactive plot with plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    emotions = sorted(emotion_counts['emotion_label'].unique())\n",
    "\n",
    "    # Color mapping for emotions\n",
    "    emotion_colors_map = {\n",
    "        'anger': '#d62728',      # red\n",
    "        'disgust': '#2ca02c',    # green\n",
    "        'fear': '#9467bd',       # purple\n",
    "        'joy': '#ff7f0e',        # orange\n",
    "        'neutral': '#7f7f7f',    # gray\n",
    "        'sadness': '#1f77b4',    # blue\n",
    "        'surprise': '#e377c2'    # pink\n",
    "    }\n",
    "\n",
    "    for emotion in emotions:\n",
    "        emotion_data = emotion_counts[emotion_counts['emotion_label'] == emotion]\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=emotion_data['time_period'],\n",
    "            y=emotion_data['percentage'],\n",
    "            name=emotion.capitalize(),\n",
    "            mode='lines+markers',\n",
    "            line=dict(color=emotion_colors_map.get(emotion, '#000000'), width=2),\n",
    "            marker=dict(size=6),\n",
    "            hovertemplate=f'<b>{emotion.capitalize()}</b><br>' +\n",
    "                         'Date: %{x}<br>' +\n",
    "                         'Percentage: %{y:.1f}%<br>' +\n",
    "                         '<extra></extra>'\n",
    "        ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'{dataset_name} - Emotion Trends Over Time',\n",
    "            font=dict(size=20, family='Arial Black')\n",
    "        ),\n",
    "        xaxis_title='Time Period',\n",
    "        yaxis_title='Percentage of Posts (%)',\n",
    "        hovermode='x unified',\n",
    "        height=600,\n",
    "        width=1200,\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            title='Emotions',\n",
    "            orientation='v',\n",
    "            yanchor='top',\n",
    "            y=1,\n",
    "            xanchor='left',\n",
    "            x=1.02\n",
    "        ),\n",
    "        margin=dict(l=60, r=150, t=80, b=60)\n",
    "    )\n",
    "\n",
    "    # Save as HTML\n",
    "    filename = f\"{dataset_name.lower()}_emotion_timeseries.html\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    fig.write_html(filepath)\n",
    "\n",
    "    print(f\"  Saved: {filename}\")\n",
    "\n",
    "    # Also create absolute count version\n",
    "    fig_count = go.Figure()\n",
    "\n",
    "    for emotion in emotions:\n",
    "        emotion_data = emotion_counts[emotion_counts['emotion_label'] == emotion]\n",
    "\n",
    "        fig_count.add_trace(go.Scatter(\n",
    "            x=emotion_data['time_period'],\n",
    "            y=emotion_data['count'],\n",
    "            name=emotion.capitalize(),\n",
    "            mode='lines+markers',\n",
    "            line=dict(color=emotion_colors_map.get(emotion, '#000000'), width=2),\n",
    "            marker=dict(size=6),\n",
    "            stackgroup='one',  # Stack the areas\n",
    "            hovertemplate=f'<b>{emotion.capitalize()}</b><br>' +\n",
    "                         'Date: %{x}<br>' +\n",
    "                         'Count: %{y}<br>' +\n",
    "                         '<extra></extra>'\n",
    "        ))\n",
    "\n",
    "    fig_count.update_layout(\n",
    "        title=dict(\n",
    "            text=f'{dataset_name} - Emotion Counts Over Time (Stacked)',\n",
    "            font=dict(size=20, family='Arial Black')\n",
    "        ),\n",
    "        xaxis_title='Time Period',\n",
    "        yaxis_title='Number of Posts',\n",
    "        hovermode='x unified',\n",
    "        height=600,\n",
    "        width=1200,\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            title='Emotions',\n",
    "            orientation='v',\n",
    "            yanchor='top',\n",
    "            y=1,\n",
    "            xanchor='left',\n",
    "            x=1.02\n",
    "        ),\n",
    "        margin=dict(l=60, r=150, t=80, b=60)\n",
    "    )\n",
    "\n",
    "    filename_count = f\"{dataset_name.lower()}_emotion_timeseries_stacked.html\"\n",
    "    filepath_count = os.path.join(save_dir, filename_count)\n",
    "    fig_count.write_html(filepath_count)\n",
    "\n",
    "    print(f\"  Saved: {filename_count}\")\n",
    "    print(f\" {dataset_name} time-series complete\\n\")\n",
    "\n",
    "print(\" Time-series function ready\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYe7TcLOZlD8",
    "outputId": "78db83f6-7a66-46b1-db7b-3f3ae922677c"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Time-series function ready\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\" GENERATING EMOTION VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate word clouds for both datasets\n",
    "for name, df in dfs.items():\n",
    "    create_emotion_wordclouds(df, name, emotion_dir)\n",
    "\n",
    "# Generate time-series for both datasets\n",
    "create_emotion_timeseries(twitter_df, 'Twitter', 'created_at', emotion_dir)\n",
    "create_emotion_timeseries(reddit_df, 'Reddit', 'created_utc', emotion_dir)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" ALL VISUALIZATIONS GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n Files saved to: {emotion_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  Word Clouds (PNG):\")\n",
    "print(\"    - twitter_[emotion]_wordcloud.png (7 files)\")\n",
    "print(\"    - reddit_[emotion]_wordcloud.png (7 files)\")\n",
    "print(\"  Time-Series (HTML):\")\n",
    "print(\"    - twitter_emotion_timeseries.html\")\n",
    "print(\"    - twitter_emotion_timeseries_stacked.html\")\n",
    "print(\"    - reddit_emotion_timeseries.html\")\n",
    "print(\"    - reddit_emotion_timeseries_stacked.html\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_DnlYlGZmy2",
    "outputId": "aefaa838-01af-4723-aea4-f6afefca364b"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      " GENERATING EMOTION VISUALIZATIONS\n",
      "============================================================\n",
      "\n",
      " Creating word clouds for Twitter...\n",
      "   Saved: twitter_anger_wordcloud.png\n",
      "   Saved: twitter_disgust_wordcloud.png\n",
      "   Saved: twitter_fear_wordcloud.png\n",
      "   Saved: twitter_joy_wordcloud.png\n",
      "   Saved: twitter_neutral_wordcloud.png\n",
      "   Saved: twitter_sadness_wordcloud.png\n",
      "   Saved: twitter_surprise_wordcloud.png\n",
      "   Saved: twitter_all_emotions_wordcloud.png\n",
      " Twitter word clouds complete\n",
      "\n",
      "\n",
      " Creating word clouds for Reddit...\n",
      "   Saved: reddit_anger_wordcloud.png\n",
      "   Saved: reddit_disgust_wordcloud.png\n",
      "   Saved: reddit_fear_wordcloud.png\n",
      "   Saved: reddit_joy_wordcloud.png\n",
      "   Saved: reddit_neutral_wordcloud.png\n",
      "   Saved: reddit_sadness_wordcloud.png\n",
      "   Saved: reddit_surprise_wordcloud.png\n",
      "   Saved: reddit_all_emotions_wordcloud.png\n",
      " Reddit word clouds complete\n",
      "\n",
      "\n",
      " Creating time-series for Twitter...\n",
      "  Saved: twitter_emotion_timeseries.html\n",
      "  Saved: twitter_emotion_timeseries_stacked.html\n",
      " Twitter time-series complete\n",
      "\n",
      "\n",
      " Creating time-series for Reddit...\n",
      "  Saved: reddit_emotion_timeseries.html\n",
      "  Saved: reddit_emotion_timeseries_stacked.html\n",
      " Reddit time-series complete\n",
      "\n",
      "============================================================\n",
      " ALL VISUALIZATIONS GENERATED\n",
      "============================================================\n",
      "\n",
      " Files saved to: /content/drive/MyDrive/ClimateLens/visualizations/emotions\n",
      "\n",
      "Generated files:\n",
      "  Word Clouds (PNG):\n",
      "    - twitter_[emotion]_wordcloud.png (7 files)\n",
      "    - reddit_[emotion]_wordcloud.png (7 files)\n",
      "  Time-Series (HTML):\n",
      "    - twitter_emotion_timeseries.html\n",
      "    - twitter_emotion_timeseries_stacked.html\n",
      "    - reddit_emotion_timeseries.html\n",
      "    - reddit_emotion_timeseries_stacked.html\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_summary_report(dfs, save_dir):\n",
    "    \"\"\"Create an HTML summary report of emotion analysis.\"\"\"\n",
    "\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Emotion Analysis Summary</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }}\n",
    "            h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n",
    "            h2 {{ color: #34495e; margin-top: 30px; }}\n",
    "            .dataset {{ background-color: white; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "            th {{ background-color: #3498db; color: white; }}\n",
    "            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
    "            .metric {{ display: inline-block; margin: 10px 20px 10px 0; }}\n",
    "            .metric-value {{ font-size: 24px; font-weight: bold; color: #3498db; }}\n",
    "            .metric-label {{ color: #7f8c8d; font-size: 14px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1> Emotion Analysis Summary Report</h1>\n",
    "        <p><strong>Generated:</strong> {timestamp}</p>\n",
    "    \"\"\".format(timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        emotion_dist = df['emotion_label'].value_counts()\n",
    "        total = len(df)\n",
    "\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"dataset\">\n",
    "            <h2> {name} Dataset</h2>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{total:,}</div>\n",
    "                <div class=\"metric-label\">Total Posts</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{len(emotion_dist)}</div>\n",
    "                <div class=\"metric-label\">Unique Emotions</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{emotion_dist.iloc[0]:,}</div>\n",
    "                <div class=\"metric-label\">Most Common: {emotion_dist.index[0].capitalize()}</div>\n",
    "            </div>\n",
    "\n",
    "            <h3>Emotion Distribution</h3>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Emotion</th>\n",
    "                    <th>Count</th>\n",
    "                    <th>Percentage</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "\n",
    "        for emotion, count in emotion_dist.items():\n",
    "            percentage = (count / total * 100)\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td><strong>{emotion.capitalize()}</strong></td>\n",
    "                    <td>{count:,}</td>\n",
    "                    <td>{percentage:.2f}%</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "\n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    html_content += \"\"\"\n",
    "        <div class=\"dataset\">\n",
    "            <h2> Generated Files</h2>\n",
    "            <h3>Word Clouds (PNG)</h3>\n",
    "            <ul>\n",
    "                <li>Individual emotion word clouds for Twitter (7 files)</li>\n",
    "                <li>Individual emotion word clouds for Reddit (7 files)</li>\n",
    "            </ul>\n",
    "            <h3>Time-Series (HTML - Interactive)</h3>\n",
    "            <ul>\n",
    "                <li>twitter_emotion_timeseries.html (percentage view)</li>\n",
    "                <li>twitter_emotion_timeseries_stacked.html (count view)</li>\n",
    "                <li>reddit_emotion_timeseries.html (percentage view)</li>\n",
    "                <li>reddit_emotion_timeseries_stacked.html (count view)</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    filepath = os.path.join(save_dir, 'emotion_analysis_summary.html')\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\" Summary report saved: emotion_analysis_summary.html\")\n",
    "    return filepath\n",
    "\n",
    "# Generate summary\n",
    "summary_path = create_summary_report(dfs, emotion_dir)\n",
    "print(f\"\\n View summary: {summary_path}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xfso799kaX4v",
    "outputId": "acc651aa-102b-4526-fe73-4c73648f2435"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Summary report saved: emotion_analysis_summary.html\n",
      "\n",
      " View summary: /content/drive/MyDrive/ClimateLens/visualizations/emotions/emotion_analysis_summary.html\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
